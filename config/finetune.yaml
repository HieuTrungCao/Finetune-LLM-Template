defaults:
- _self_
- data: data
- model: llama
- training_arg: training_arg
- trainer: trainer
- lora: lora
- qlora: qlora
# - log: "log hear"

task_name: "Finetune"

tags: ["dev"]

# set False to skip model finetuning
finetune: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

# seed for random number generators in pytorch, numpy and python.random
seed: null